// ============================================================================
// TESTE REALISTA DO ASSISTENTE DE PROMPTS
// ============================================================================
// Este script simula EXATAMENTE o que o Assistente de Prompts faz,
// mas testa diferentes configuraÃ§Ãµes para encontrar a mais rÃ¡pida.
// ============================================================================

// Gerar prompt de 21k caracteres (como o do Leandro)
function generateLargePrompt(): string {
  const instruction = `VocÃª Ã© um especialista em estratÃ©gia de vendas, relacionamento com clientes e psicologia comportamental. 
Seu objetivo Ã© ajudar vendedores a aumentar significativamente suas conversÃµes atravÃ©s de tÃ©cnicas de persuasÃ£o Ã©tica.

CONHECIMENTOS:
- Psicologia de vendas: AIDA, SPIN selling, tÃ©cnicas de fechamento
- Rapport: Espelhamento, sincronizaÃ§Ã£o, linguagem hipnÃ³tica
- NegociaÃ§Ã£o: Win-win, BATNA, objeÃ§Ã£o handling
- ComunicaÃ§Ã£o: Storytelling, framing, linguagem de padrÃµes

INSTRUÃ‡Ã•ES DETALHADAS:
1. Analise o contexto da venda e do cliente
2. Identifique objeÃ§Ãµes e prepare antÃ­dotos
3. Sugira passos com linguagem especÃ­fica
4. Mantenha Ã©tica e respeito ao cliente
5. Considere timing e canal de comunicaÃ§Ã£o
6. DÃª exemplos prÃ¡ticos de fala
7. Explique o 'por quÃª' das sugestÃµes

EXEMPLOS DE LINGUAGEM:
- "Eu entendo que vocÃª esteja preocupado..."
- "O que torna este produto diferente Ã©..."
- "Posso fazer uma pergunta?"
- "Imagine por um momento..."

CASOS DE USO:
1. Vendas B2B: ROI, integraÃ§Ã£o, suporte
2. ServiÃ§os: Outcomes, credibilidade
3. Consultivas: DiagnÃ³stico, soluÃ§Ã£o
4. Transacionais: ConveniÃªncia, preÃ§o`;

  let prompt = instruction;
  while (prompt.length < 21000) {
    prompt += "\n\n---\n" + instruction;
  }
  return prompt.substring(0, 21679);
}

// Construir system prompt IGUAL ao do cÃ³digo real
function buildSystemPrompt(agentInstructions: string): string {
  return `VocÃª Ã© um especialista em criar prompts para agentes de IA de vendas e atendimento.
Seu objetivo Ã© ajudar a melhorar o prompt do agente "Leandro aÃ­".

PROMPT ATUAL DO AGENTE:
"""
${agentInstructions}
"""

REGRAS:
1. Quando o usuÃ¡rio pedir melhorias, sugira alteraÃ§Ãµes especÃ­ficas e explique o porquÃª
2. Quando vocÃª propor um novo prompt, formate-o EXATAMENTE assim:
   [NOVO_PROMPT_INICIO]
   <o prompt completo aqui>
   [NOVO_PROMPT_FIM]
3. Seja especÃ­fico e prÃ¡tico nas sugestÃµes
4. Mantenha o tom profissional do agente
5. Sugira melhorias baseadas em boas prÃ¡ticas de vendas e persuasÃ£o
6. Pergunte sobre o contexto e objetivos antes de fazer grandes mudanÃ§as

Responda em portuguÃªs brasileiro.`;
}

// Mensagem do usuÃ¡rio (tÃ­pica do Assistente de Prompts)
const USER_MESSAGE = "fale mais persuasivo e inteligente";

// ConfiguraÃ§Ãµes para testar
interface TestConfig {
  name: string;
  model: string;
  maxTokens: number;
  reasoningEffort?: string;
  temperature: number;
  role: 'developer' | 'system';
}

const CONFIGS: TestConfig[] = [
  // Config atual do cÃ³digo
  {
    name: "ATUAL (gpt-5.1 + 9k tokens)",
    model: "gpt-5.1",
    maxTokens: 9227,
    reasoningEffort: "none",
    temperature: 0.7,
    role: "developer"
  },
  // Teste com menos tokens (resposta mais curta)
  {
    name: "gpt-5.1 + 4k tokens (menos)",
    model: "gpt-5.1",
    maxTokens: 4000,
    reasoningEffort: "none",
    temperature: 0.7,
    role: "developer"
  },
  // Teste com gpt-4.1 (nÃ£o-reasoning, mais rÃ¡pido)
  {
    name: "gpt-4.1 + 4k tokens",
    model: "gpt-4.1",
    maxTokens: 4000,
    temperature: 0.7,
    role: "system"
  },
  // Teste com gpt-4.1-mini (mais rÃ¡pido ainda)
  {
    name: "gpt-4.1-mini + 4k tokens",
    model: "gpt-4.1-mini",
    maxTokens: 4000,
    temperature: 0.7,
    role: "system"
  },
  // Teste com gpt-4o (modelo padrÃ£o, bom equilÃ­brio)
  {
    name: "gpt-4o + 4k tokens",
    model: "gpt-4o",
    maxTokens: 4000,
    temperature: 0.7,
    role: "system"
  }
];

interface TestResult {
  config: string;
  time: number;
  tokens: number;
  responseLength: number;
  success: boolean;
  error?: string;
}

async function testConfig(
  config: TestConfig,
  systemPrompt: string,
  userMessage: string,
  apiKey: string
): Promise<TestResult> {
  const startTime = Date.now();

  try {
    const body: any = {
      model: config.model,
      messages: [
        { role: config.role, content: systemPrompt },
        { role: "user", content: userMessage }
      ],
      temperature: config.temperature
    };

    // GPT-5.1 usa max_completion_tokens e reasoning_effort
    if (config.model.includes("5.1") || config.model.includes("o1") || config.model.includes("o3")) {
      body.max_completion_tokens = config.maxTokens;
      if (config.reasoningEffort) {
        body.reasoning_effort = config.reasoningEffort;
      }
    } else {
      // Outros modelos usam max_tokens
      body.max_tokens = config.maxTokens;
    }

    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${apiKey}`
      },
      body: JSON.stringify(body)
    });

    const endTime = Date.now();
    const elapsed = endTime - startTime;

    if (!response.ok) {
      const errorData = await response.json();
      return {
        config: config.name,
        time: elapsed,
        tokens: 0,
        responseLength: 0,
        success: false,
        error: errorData.error?.message || "Erro desconhecido"
      };
    }

    const data = await response.json();
    const responseContent = data.choices[0].message.content || "";

    return {
      config: config.name,
      time: elapsed,
      tokens: data.usage?.completion_tokens || 0,
      responseLength: responseContent.length,
      success: true
    };
  } catch (error: any) {
    return {
      config: config.name,
      time: Date.now() - startTime,
      tokens: 0,
      responseLength: 0,
      success: false,
      error: error.message
    };
  }
}

async function main() {
  const apiKey = process.env.OPENAI_API_KEY;

  if (!apiKey) {
    console.log("âŒ OPENAI_API_KEY nÃ£o definida!");
    console.log("   Use: $env:OPENAI_API_KEY='sua-chave'; npx tsx scripts/test-assistant-performance.ts");
    process.exit(1);
  }

  console.log("\n");
  console.log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
  console.log("â•‘  TESTE DE PERFORMANCE DO ASSISTENTE DE PROMPTS                 â•‘");
  console.log("â•‘  Prompt: 21.679 caracteres (igual ao do Leandro)               â•‘");
  console.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

  // Gerar prompt de 21k
  const agentInstructions = generateLargePrompt();
  const systemPrompt = buildSystemPrompt(agentInstructions);

  console.log(`ğŸ“ InstruÃ§Ãµes do agente: ${agentInstructions.length.toLocaleString()} caracteres`);
  console.log(`ğŸ“‹ System prompt total: ${systemPrompt.length.toLocaleString()} caracteres`);
  console.log(`ğŸ’¬ Mensagem do usuÃ¡rio: "${USER_MESSAGE}"\n`);

  console.log("â•".repeat(70));
  console.log("Testando cada configuraÃ§Ã£o...\n");

  const results: TestResult[] = [];

  for (const config of CONFIGS) {
    process.stdout.write(`ğŸ§ª ${config.name}... `);
    const result = await testConfig(config, systemPrompt, USER_MESSAGE, apiKey);
    results.push(result);

    if (result.success) {
      console.log(
        `âœ… ${(result.time / 1000).toFixed(2)}s | ${result.tokens} tokens | ${result.responseLength} chars`
      );
    } else {
      console.log(`âŒ ${result.error}`);
    }

    // Esperar 1s entre testes para nÃ£o sobrecarregar
    await new Promise((r) => setTimeout(r, 1000));
  }

  // Ordenar por tempo
  const successResults = results.filter((r) => r.success);
  successResults.sort((a, b) => a.time - b.time);

  console.log("\n" + "â•".repeat(70));
  console.log("ğŸ“Š RANKING (mais rÃ¡pido primeiro):\n");

  successResults.forEach((r, i) => {
    const medal = i === 0 ? "ğŸ¥‡" : i === 1 ? "ğŸ¥ˆ" : i === 2 ? "ğŸ¥‰" : "  ";
    const timeStr = `${(r.time / 1000).toFixed(2)}s`.padEnd(8);
    const tokensStr = `${r.tokens} tokens`.padEnd(12);
    console.log(`${medal} ${timeStr} | ${tokensStr} | ${r.config}`);
  });

  console.log("\n" + "â•".repeat(70));

  if (successResults.length > 0) {
    const fastest = successResults[0];
    const current = results.find((r) => r.config.includes("ATUAL"));

    console.log("\nğŸ¯ RECOMENDAÃ‡ÃƒO:\n");

    if (current && current.success && fastest.config !== current.config) {
      const speedup = ((current.time - fastest.time) / current.time * 100).toFixed(0);
      console.log(`   ConfiguraÃ§Ã£o atual: ${(current.time / 1000).toFixed(2)}s`);
      console.log(`   ConfiguraÃ§Ã£o mais rÃ¡pida: ${(fastest.time / 1000).toFixed(2)}s`);
      console.log(`   Melhoria: ${speedup}% mais rÃ¡pido! âš¡\n`);
      console.log(`   ğŸ‘‰ Recomendo trocar para: ${fastest.config.split(" + ")[0]}`);
    } else if (fastest.config.includes("ATUAL")) {
      console.log("   âœ… A configuraÃ§Ã£o atual jÃ¡ Ã© a mais rÃ¡pida!");
    }
  }

  console.log("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
  console.log("â•‘  TESTE COMPLETO                                                â•‘");
  console.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
}

main().catch(console.error);
